#Implementation of the gradient descent algorithm and how to use it in code

import numpy as np

#X = [x1, x2, ......., xm] is a 2D array
#y = [y1, y2, .........., ym] where m = num_of_examples/samples

class LinearRegressionGD:
  def __init__(self, learning_rate = 0.01, n_iterations = 1000):
    self.learning_rate = learning_rate
    self.n_iterations = n_iterations
    self.weights = None
    self.bias = None

  def fit(self, X, y):
    #Initialize the weights and biases

    self.weights = np.zeros(X.shape[1]) #0 weight for each columns aka features
    self.bias = 0

    #Gradient Descent

    m = X.shape[0]
    for _ in range(self.n_iterations):

      #Make predictions
      y_pred = self.predict(X)

      #compute gradients

      d_weights = (1/m) * np.dot(X.T, (y_pred - y))
      d_bias = (-1/m) * np.sum((y_pred-y))

      self.weights -= self.learning_rate * d_weights
      self. bias -= self.learning_rate * d_bias

  def predict(self, X):
    return np.dot(self.weights, X) + self.bias

  def mean_squared_error (self, y_true, y_pred):
    m = len(y_true)
    return (-1/(2*m))* np.sum((y_pred-y_true)**2)

  def score(self, X, y):
    y_pred = self.predict(X)
    mse = self.mean_squared_error(y, y_pred)
    return mse

X = np.array([[1, 2], [2, 3], [4, 5]])  # 3 samples, 2 features
y = np.array([1, 2, 3])

model = LinearRegressionGD()
model.fit(X, y)
predictions = model.predict(X)
mse = model.mean_squared_error(y, predictions)
print("Predictions:", predictions)
print("Mean Squared Error:", mse)
